{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import LLMMathChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), \n",
    "                openai_api_base=\"https://free-cdo.openai.azure.com/openai/deployments/cod-free-gpt4o/chat/completions?api-version=2024-02-15-preview\",\n",
    "                openai_api_version=\"2024-02-15-preview\",\n",
    "                openai_api_type=\"azure\",\n",
    "                temperature=0.2\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = WikipediaAPIWrapper()\n",
    "\n",
    "wikipedia_tool = Tool(\n",
    "                    name=\"Wikipedia\",\n",
    "                    func=wikipedia.run,\n",
    "\t                description=\"A useful tool for searching the Internet to find information on world events, \\\n",
    "                               issues, dates, years, etc. Worth using for general topics. Use precise questions.\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_chain = LLMMathChain.from_llm(llm=llm)\n",
    "\n",
    "math_tool = Tool.from_function(name=\"Calculator\",\n",
    "                func=problem_chain.run,\n",
    "                description=\"Useful for when you need to answer questions about math. \\\n",
    "                    This tool is only for math questions and nothing else. Only inputmath expressions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mn9lu5b/Library/CloudStorage/OneDrive-Allianz/Github/Team AZ Account/genai-quickstart/quick_start/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "word_problem_template = \"\"\"You are a reasoning agent tasked with solving \n",
    "the user's logic-based questions. Logically arrive at the solution, and be \n",
    "factual. In your answers, clearly detail the steps involved and give the \n",
    "final answer. Provide the response in bullet points. \n",
    "Question {question} Answer\"\"\"\n",
    "\n",
    "math_assistant_prompt = PromptTemplate(input_variables=[\"question\"],\n",
    "                                       template=word_problem_template\n",
    "                                       )\n",
    "word_problem_chain = LLMChain(llm=llm,\n",
    "                              prompt=math_assistant_prompt)\n",
    "\n",
    "word_problem_tool = Tool.from_function(name=\"Reasoning Tool\",\n",
    "                                       func=word_problem_chain.run,\n",
    "                                       description=\"Useful for when you need to answer logic-based/reasoning questions.\",\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to determine the total number of fruits after performing the given actions. I'll break down the problem step by step.\n",
      "\n",
      "1. Start with 3 apples and 4 oranges.\n",
      "2. Give away half of the oranges.\n",
      "3. Buy two dozen new oranges.\n",
      "4. Buy three packs of strawberries, each containing 30 strawberries.\n",
      "\n",
      "First, I'll calculate the number of oranges after giving away half and then adding two dozen new ones. Then, I'll calculate the total number of strawberries. Finally, I'll sum all the fruits together.\n",
      "\n",
      "Action: Calculator\n",
      "Action Input: 4 / 2\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know that half of the oranges (4) is 2. So, after giving away half, I have 2 oranges left. Next, I need to add two dozen (24) new oranges to these 2 oranges.\n",
      "\n",
      "Action: Calculator\n",
      "Action Input: 2 + 24\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 26\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know that after giving away half of the oranges and buying two dozen new ones, I have a total of 26 oranges. Next, I need to calculate the total number of strawberries by multiplying the number of packs by the number of strawberries per pack.\n",
      "\n",
      "Action: Calculator\n",
      "Action Input: 3 * 30\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 90\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know that I have 90 strawberries. To find the total number of fruits, I need to sum the apples, oranges, and strawberries.\n",
      "\n",
      "- Apples: 3\n",
      "- Oranges: 26\n",
      "- Strawberries: 90\n",
      "\n",
      "Action: Calculator\n",
      "Action Input: 3 + 26 + 90\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 119\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "\n",
      "Final Answer: You have a total of 119 pieces of fruit at the end.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'I have 3 apples and 4 oranges. I give half of my oranges                away and buy two dozen new ones, alongwith three packs of                strawberries. Each pack of strawberry has 30 strawberries.                How  many total pieces of fruit do I have at the end?', 'output': 'You have a total of 119 pieces of fruit at the end.'}\n"
     ]
    }
   ],
   "source": [
    "agent = initialize_agent(\n",
    "            tools=[wikipedia_tool, math_tool, word_problem_tool],\n",
    "            llm=llm,\n",
    "            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "            verbose=True,\n",
    "            handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "print(agent.invoke({\"input\": \"I have 3 apples and 4 oranges. I give half of my oranges \\\n",
    "               away and buy two dozen new ones, alongwith three packs of \\\n",
    "               strawberries. Each pack of strawberry has 30 strawberries. \\\n",
    "               How  many total pieces of fruit do I have at the end?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unrelated tests for web scraping..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "\n",
    "# Load HTML\n",
    "loader = AsyncChromiumLoader([\"https://www.wsj.com\"])\n",
    "html = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "bs_transformer = BeautifulSoupTransformer()\n",
    "docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=[\"span\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result\n",
    "docs_transformed[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 2/2 [00:06<00:00,  3.42s/it]\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/html/__init__.py:91: RuntimeWarning: coroutine 'AsyncChromiumLoader.ascrape_playwright' was never awaited\n",
      "  def _replace_charref(s):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/html/__init__.py:91: RuntimeWarning: coroutine 'load_html' was never awaited\n",
      "  def _replace_charref(s):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "\n",
    "urls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\n",
    "\n",
    "loader = AsyncHtmlLoader(urls)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skip to main content  Skip to navigation\\n\\n<\\n\\n>\\n\\nMenu\\n\\n## ESPN\\n\\n  *   *   *   * scores\\n\\nNEW! Find where to watch all of your favorite sports!\\n\\n  * NFL\\n  * NBA\\n  * MLB\\n  * NCAAF\\n  * NHL\\n  * Soccer\\n  * WNBA\\n  * More Sports\\n\\n    * Boxing\\n    * CFL\\n    * NCAA\\n    * Cricket\\n    * F1\\n    * Golf\\n    * Horse\\n    * LLWS\\n    * MMA\\n    * NASCAR\\n    * NBA G League\\n    * NBA Summer League\\n    * NCAAM\\n    * NCAAW\\n    * NWSL\\n    * Olympics\\n    * PLL\\n    * Professional Wrestling\\n    * Racing\\n    * RN BB\\n    * RN'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "\n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)\n",
    "docs_transformed[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), \n",
    "                openai_api_base=\"https://free-cdo.openai.azure.com/openai/deployments/cod-free-gpt4o/chat/completions?api-version=2024-02-15-preview\",\n",
    "                openai_api_version=\"2024-02-15-preview\",\n",
    "                openai_api_type=\"azure\",\n",
    "                temperature=0.2\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebBrowserTool(BaseTool):\n",
    "    name = \"Web Browser\"\n",
    "    description = \"A tool for browsing and extracting information from websites.\"\n",
    "\n",
    "    def _run(self, url: str) -> str:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract text content from the page\n",
    "        text_content = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Limit the content to a reasonable length\n",
    "        return text_content[:5000]  # Adjust as needed\n",
    "\n",
    "    def _arun(self, url: str) -> str:\n",
    "        # For asynchronous operations\n",
    "        raise NotImplementedError(\"WebBrowserTool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "\n",
    "web_browser = WebBrowserTool()\n",
    "tools = load_tools([\"llm-math\"], llm=llm) + [web_browser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mn9lu5b/Library/CloudStorage/OneDrive-Allianz/Github/Team AZ Account/genai-quickstart/quick_start/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mn9lu5b/Library/CloudStorage/OneDrive-Allianz/Github/Team AZ Account/genai-quickstart/quick_start/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: Summarize the content of https://www.example.com\n",
      "Thought: I need to visit the website to gather information for the summary.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Web Browser\",\n",
      "  \"action_input\": \"https://www.example.com\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mExample Domain Example Domain This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission. More information...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe content of https://www.example.com is a placeholder website used for illustrative purposes in documents. It states that the domain is available for use in literature without needing prior coordination or permission.\n",
      "\n",
      "Final Answer: The content of https://www.example.com is a placeholder website used for illustrative purposes in documents, and it can be used without prior coordination or permission.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The content of https://www.example.com is a placeholder website used for illustrative purposes in documents, and it can be used without prior coordination or permission.\n"
     ]
    }
   ],
   "source": [
    "# Use the agent to browse a website and extract information\n",
    "result = agent.run(\"Summarize the content of https://www.example.com\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "            tools=[wikipedia_tool, math_tool, word_problem_tool, web_browser],\n",
    "            llm=llm,\n",
    "            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "            verbose=True,\n",
    "            handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# result = agent.invoke({\"input\": \"I have 3 apples and 4 oranges. I give half of my oranges \\\n",
    "#                away and buy two dozen new ones, alongwith three packs of \\\n",
    "#                strawberries. Each pack of strawberry has 30 strawberries. \\\n",
    "#                How  many total pieces of fruit do I have at the end?\"})\n",
    "# print(result)\n",
    "\n",
    "result = agent.run(\"Summarize the content of https://www.example.com\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ NEW Vantage Launches Usage-Based Reporting -> ](/blog/vantage-launches-\n",
      "usage-based-reporting)\n",
      "\n",
      "[ ![](/images/nav-logo.svg) ](/) [We're Hiring!](/careers)\n",
      "\n",
      "  * Features expand_more\n",
      "  * Integrations expand_more\n",
      "  * Resources expand_more\n",
      "  * [ Partners ](/partners)\n",
      "  * [ Pricing ](/pricing)\n",
      "  * [ Docs ](https://docs.vantage.sh/)\n",
      "\n",
      "    * Visibility\n",
      "    * [ article Cost Reporting Easily build complex reports ](/features/cost-reports)\n",
      "    * [ ![](/images/icons/icon-k8.svg) Kubernetes Monitoring and efficiency metrics ](/features/kubernetes)\n",
      "    * [ sell Virtual Tagging Custom cost allocation tags ](/features/virtual-tagging)\n",
      "    * [ swap_horiz Network Flow Reports Network cost visibility ](/features/network-flow-reports)\n",
      "    * [ account_tree Cost Allocation Organizational cost hierarchies ](/features/segments)\n",
      "    * [ dialpad Budgeting Budgeting and budget alerts ](/features/budgets)\n",
      "    * [ category Resource Reports Discover active resources ](/features/resource-inventory)\n",
      "    * [ data_usage Usage-Based Reporting Consumption-based insights ](/features/usage-based-reporting)\n",
      "    * [ nearby_error Anomaly Detection Alerts for unexpected charges ](/features/cost-anomaly-alerts)\n",
      "    * Optimization\n",
      "    * [ flight_takeoff Autopilot for Savings Plans Automated AWS cost savings ](/features/autopilot)\n",
      "    * [ checklist Cost Recommendations Discover cost savings ](/features/cost-recommendations)\n",
      "    * [ summarize Commitment Reports Unified view of AWS discounts ](/features/financial-commitment-reports)\n",
      "    * FinOps\n",
      "    * [ calculate Unit Costs COGS and business metrics ](/features/unit-costs)\n",
      "    * [ table_view Savings Planner Model savings plans ](/features/financial-planning)\n",
      "    * [ adjust Issues Collaborate on cost initiatives ](/features/issues)\n",
      "    * [ supervised_user_circle Team Access Create and manage your teams ](/features/team-management)\n",
      "    * [ add_task Terraform Automate cloud infrastructure ](/features/terraform)\n",
      "\n",
      "[ All Features chevron_right ](/features)\n",
      "\n",
      "    * [ ![](/images/icons/24/icon-24-aws.svg) AWS ](/integrations/aws)\n",
      "    * [ ![](/images/icons/24/icon-24-azure.svg) Azure ](/integrations/azure)\n",
      "    * [ ![](/images/icons/24/icon-24-gcp.svg) Google Cloud ](/integrations/google-cloud)\n",
      "    * [ ![](/images/icons/24/icon-24-oracle.svg) Oracle Cloud ](/integrations/oracle-cloud)\n",
      "    * [ ![](/images/icons/24/icon-24-kubernetes.svg) Kubernetes ](/integrations/kubernetes)\n",
      "    * [ ![](/images/icons/24/icon-24-datadog.svg) Datadog ](/integrations/datadog)\n",
      "    * [ ![](/images/icons/24/icon-24-fastly.svg) Fastly ](/integrations/fastly)\n",
      "    * [ ![](/images/icons/24/icon-24-mongodb.svg) MongoDB ](/integrations/mongodb)\n",
      "    * [ ![](/images/icons/24/icon-24-snowflake.svg) Snowflake ](/integrations/snowflake)\n",
      "    * [ ![](/images/icons/24/icon-24-databricks.svg) Databricks ](/integrations/databricks)\n",
      "    * [ ![](/images/icons/24/icon-24-new-relic.svg) New Relic ](/integrations/new-relic)\n",
      "    * [ ![](/images/icons/24/icon-24-confluent.svg) Confluent ](/integrations/confluent)\n",
      "    * [ ![](/images/icons/24/icon-24-planetscale.svg) PlanetScale ](/integrations/planetscale)\n",
      "    * [ ![](/images/icons/24/icon-24-coralogix.svg) Coralogix ](/integrations/coralogix)\n",
      "    * [ ![](/images/icons/24/icon-24-github.svg) GitHub ](/integrations/github)\n",
      "    * [ ![](/images/icons/24/icon-24-custom-providers.svg) Custom Providers ](/integrations/custom-providers)\n",
      "\n",
      "[ All Integrations chevron_right ](/integrations)\n",
      "\n",
      "    * Company\n",
      "    * [ group About ](/about)\n",
      "    * [ rss_feed Blog ](/blog)\n",
      "    * [ favorite Customers ](/customers)\n",
      "    * [ headphones Podcasts & Talks ](/podcasts)\n",
      "    * [ newspaper Newsroom ](/newsroom)\n",
      "    * Community\n",
      "    * [ celebration Events ](/events)\n",
      "    * ###### [Slack Community ](https://vantage.sh/slack)\n",
      "    * [ dns EC2Instances.info ](https://instances.vantage.sh)\n",
      "    * [ description Cloud Cost Reports ](/cloud-cost-report)\n",
      "    * [ auto_stories Cloud Cost Handbook ](https://handbook.vantage.sh)\n",
      "    * [ scoreboard Cloud Cost Leaderboard ](https://leaderboard.vantage.sh/)\n",
      "    * Documentation\n",
      "    * [ format_list_bulleted Product Changelog ](https://docs.vantage.sh/changelog)\n",
      "    * [ school Vantage University ](https://docs.vantage.sh/vantage_university)\n",
      "    * [ api API Documentation ](https://vantage.readme.io/)\n",
      "\n",
      "menu\n",
      "\n",
      "[Login](https://console.vantage.sh/login) [Get a Demo](/contact/sales) [Get\n",
      "Started](https://console.vantage.sh/signup)\n",
      "\n",
      "[ ![](/images/nav-logo.svg) ](/) close\n",
      "\n",
      "[We're Hiring!](/careers)\n",
      "\n",
      "[Get a Demo](/contact/sales) [Get Started](https://console.vantage.sh/signup)\n",
      "\n",
      "  * [ Login ](https://console.vantage.sh/login)\n",
      "  * Features chevron_right\n",
      "  * Integrations chevron_right\n",
      "  * Resources chevron_right\n",
      "  * [ Partners ](/partners)\n",
      "  * [ Pricing ](/pricing)\n",
      "  * [ Docs ](https://docs.vantage.sh)\n",
      "\n",
      "chevron_left Back\n",
      "\n",
      "close\n",
      "\n",
      "  * Visibility\n",
      "  * [ article Cost Reporting Easily build complex reports ](/features/cost-reports)\n",
      "  * [ ![](/images/icons/icon-k8.svg) Kubernetes Monitoring and efficiency metrics ](/features/kubernetes)\n",
      "  * [ sell Virtual Tagging Custom cost allocation tags ](/features/virtual-tagging)\n",
      "  * [ swap_horiz Network Flow Reports Network cost visibility ](/features/network-flow-reports)\n",
      "  * [ account_tree Cost Allocation Organizational cost hierarchies ](/features/segments)\n",
      "  * [ dialpad Budgeting Budgeting and budget alerts ](/features/budgets)\n",
      "  * [ category Resource Reports Discover active resources ](/features/resource-inventory)\n",
      "  * [ data_usage Usage-Based Reporting Consumption-based insights ](/features/usage-based-reporting)\n",
      "  * [ nearby_error Anomaly Detection Detect cost spikes ](/features/cost-anomaly-alerts)\n",
      "\n",
      "  * Optimization\n",
      "  * [ flight_takeoff Autopilot for Savings Plans Automated AWS cost savings ](/features/autopilot)\n",
      "  * [ checklist Cost Recommendations Discover cost savings ](/features/cost-recommendations)\n",
      "  * [ summarize Commitment Reports Unified view of AWS discounts ](/features/financial-commitment-reports)\n",
      "\n",
      "  * FinOps\n",
      "  * [ calculate Unit Costs COGS and business metrics ](/features/unit-costs)\n",
      "  * [ table_view Savings Planner Model savings plans ](/features/financial-planning)\n",
      "  * [ adjust Issues Collaborate on cost initiatives ](/features/issues)\n",
      "  * [ supervised_user_circle Team Access Create and manage your teams ](/features/team-management)\n",
      "  * [ add_task Terraform Automate cloud infrastructure ](/features/terraform)\n",
      "\n",
      "[ All Features chevron_right ](/features)\n",
      "\n",
      "chevron_left Back\n",
      "\n",
      "close\n",
      "\n",
      "  * Integrations\n",
      "  * [ ![](/images/icons/24/icon-24-aws.svg) AWS ](/integrations/aws)\n",
      "  * [ ![](/images/icons/24/icon-24-azure.svg) Azure ](/integrations/azure)\n",
      "  * [ ![](/images/icons/24/icon-24-gcp.svg) Google Cloud ](/integrations/google-cloud)\n",
      "  * [ ![](/images/icons/24/icon-24-oracle.svg) Oracle Cloud ](/integrations/oracle-cloud)\n",
      "  * [ ![](/images/icons/24/icon-24-kubernetes.svg) Kubernetes ](/integrations/kubernetes)\n",
      "  * [ ![](/images/icons/24/icon-24-fastly.svg) Fastly ](/integrations/fastly)\n",
      "  * [ ![](/images/icons/24/icon-24-datadog.svg) Datadog ](/integrations/datadog)\n",
      "  * [ ![](/images/icons/24/icon-24-mongodb.svg) MongoDB ](/integrations/mongodb)\n",
      "  * [ ![](/images/icons/24/icon-24-snowflake.svg) Snowflake ](/integrations/snowflake)\n",
      "  * [ ![](/images/icons/24/icon-24-databricks.svg) Databricks ](/integrations/databricks)\n",
      "  * [ ![](/images/icons/24/icon-24-new-relic.svg) New Relic ](/integrations/new-relic)\n",
      "  * [ ![](/images/icons/24/icon-24-confluent.svg) Confluent ](/integrations/confluent)\n",
      "  * [ ![](/images/icons/24/icon-24-planetscale.svg) PlanetScale ](/integrations/planetscale)\n",
      "  * [ ![](/images/icons/24/icon-24-coralogix.svg) Coralogix ](/integrations/coralogix)\n",
      "  * [ ![](/images/icons/24/icon-24-github.svg) GitHub ](/integrations/github)\n",
      "  * [ ![](/images/icons/24/icon-24-custom-providers.svg) Custom Providers ](/integrations/custom-providers)\n",
      "\n",
      "[ All Integrations chevron_right ](/integrations)\n",
      "\n",
      "chevron_left Back\n",
      "\n",
      "close\n",
      "\n",
      "    *     * Company\n",
      "    * [ group About ](/about)\n",
      "    * [ rss_feed Blog ](/blog)\n",
      "    * [ favorite Customers ](/customers)\n",
      "    * [ headphones Podcasts & Talks ](/podcasts)\n",
      "    * [ newspaper Newsroom ](/newsroom)\n",
      "    * Community\n",
      "    * [ celebration Events ](/events)\n",
      "    * ###### [Slack Community ](https://vantage.sh/slack)\n",
      "    * [ dns EC2Instances.info ](https://instances.vantage.sh)\n",
      "    * [ description Cloud Cost Reports ](/cloud-cost-report)\n",
      "    * [ auto_stories Cloud Cost Handbook ](https://handbook.vantage.sh)\n",
      "    * [ scoreboard Cloud Cost Leaderboard ](https://leaderboard.vantage.sh/)\n",
      "    * Documentation\n",
      "    * [ format_list_bulleted Product Changelog ](https://docs.vantage.sh/changelog)\n",
      "    * [ school Vantage University ](https://docs.vantage.sh/vantage_university)\n",
      "    * [ api API Documentation ](https://vantage.readme.io/)\n",
      "\n",
      "Content • Jun 28, 2024\n",
      "\n",
      "# Amazon Bedrock vs Azure OpenAI: Pricing Considerations\n",
      "\n",
      "by Emily Dunenfeld\n",
      "\n",
      "Amazon Bedrock shakes up the Generative AI landscape with highly cost\n",
      "competitive models.\n",
      "\n",
      "  * [ ![twitter](https://assets.vantage.sh/blog/icons/purple_twitter.png) ](https://twitter.com/intent/tweet?text=Amazon Bedrock vs Azure OpenAI: Pricing Considerations https://www.vantage.sh/blog/azure-openai-vs-amazon-bedrock-cost via @joinvantage)\n",
      "  * [ ![LinkedIn](https://assets.vantage.sh/blog/icons/purple_linkedin.png) ](https://www.linkedin.com/sharing/share-offsite/?url=https://www.vantage.sh/blog/azure-openai-vs-amazon-bedrock-cost)\n",
      "  * [ ![rss](https://assets.vantage.sh/blog/icons/purple_rss.png) ](/feed/Content.xml)\n",
      "  * [ ![Hacker News](https://assets.vantage.sh/blog/icons/purple_hackernews.png) ](https://news.ycombinator.com/submitlink?u=https://www.vantage.sh/blog/azure-openai-vs-amazon-bedrock-cost&t=Amazon Bedrock vs Azure OpenAI: Pricing Considerations)\n",
      "  * [ ![Reddit](https://assets.vantage.sh/blog/icons/purple_reddit.png) ](https://www.reddit.com/submit?url=https://www.vantage.sh/blog/azure-openai-vs-amazon-bedrock-cost&title=Amazon Bedrock vs Azure OpenAI: Pricing Considerations)\n",
      "\n",
      "![](https://assets.vantage.sh/blog/bedrock-vs-openai/bedrock-vs-openai-3.jpg)\n",
      "\n",
      "Contents\n",
      "\n",
      "  * Types of Generative AI Models Available in the Cloud\n",
      "  * Amazon Bedrock\n",
      "    * Bedrock Supported Models\n",
      "  * Azure OpenAI\n",
      "    * OpenAI Supported Models\n",
      "  * Amazon Bedrock vs Azure OpenAI Functionality\n",
      "    * Service Comparison\n",
      "    * Model Comparison\n",
      "  * Bedrock Pricing\n",
      "    * On-Demand\n",
      "    * Provisioned Throughput\n",
      "    * Model Customization\n",
      "  * Azure OpenAI Pricing\n",
      "    * Pay-As-You-Go\n",
      "    * Model Customization\n",
      "  * Pricing Comparison: Bedrock vs OpenAI\n",
      "    * Standard Context Window: Command, Llama 2, Titan Text vs GPT-3.5 Turbo 4k\n",
      "      * Chatbot Scenario: Titan Text Express vs GPT-3.5 Turbo 4k\n",
      "    * Long Context Window: Long Context Window: Jurassic-2 vs GPT-4 8k\n",
      "    * Long Context Window: Claude Instant vs GPT-3.5 Turbo 16k\n",
      "    * Extra Long Context Window: Claude 2.1 vs GPT-4 32k\n",
      "      * Text Summarization Scenario: Claude 2.1 vs GPT-4 32k\n",
      "  * Conclusion\n",
      "\n",
      "Amazon has been questioned repeatedly by analysts and its own customers about\n",
      "its AI efforts. A quick listen to its Q3 earnings call tells you they’re\n",
      "listening. Much of Amazon’s efforts around Generative AI have culminated in\n",
      "two important releases: [Bedrock](https://www.aboutamazon.com/news/aws/aws-\n",
      "amazon-bedrock-general-availability-generative-ai-innovations) and\n",
      "[Titan](https://aws.amazon.com/bedrock/titan/).\n",
      "\n",
      "But shipping AI and winning real-world customers are two very different\n",
      "things. How does Bedrock stack up against OpenAI? Bedrock has more model\n",
      "choices, which can correspond to cost savings.\n",
      "\n",
      "_Editor’s Note: Amazon and Azure are continuously unveiling changes and\n",
      "releases to their AI services. To ensure you receive the most up-to-date and\n",
      "accurate information, we will be updating sections of this blog regularly. As\n",
      "we make these updates, we will mark the revision date on a section-by-section\n",
      "basis._\n",
      "\n",
      "## Types of Generative AI Models Available in the Cloud\n",
      "\n",
      "[Generative\n",
      "AI](https://www.techtarget.com/searchenterpriseai/definition/generative-AI)\n",
      "encompasses several categories of models available via OpenAI and Bedrock:\n",
      "\n",
      "  * Image Generation: As the name suggests, image generation models, such as Stable Diffusion and DALL-E, are designed to create images based on textual descriptions or other image inputs.\n",
      "  * [Large Language Model](/blog/optimize-large-language-model-costs) (LLM): These models are extensively trained on vast text data and understand/produce textual content. LLMs power ChatGPT, the most popular consumer-facing Generative AI model. \n",
      "    * Base Model: A Foundation Model that serves as a starting point for further fine-tuning.\n",
      "    * Embeddings: Embeddings models are not focused on generating text, but on representing text or data in a numerical way that captures semantic relationships, e.g. “king” + “woman” = “queen”.\n",
      "    * Text Generation: Specializes in producing human-like text.\n",
      "  * Transcription: These models convert audio into text.\n",
      "\n",
      "## Amazon Bedrock\n",
      "\n",
      "[Bedrock](https://aws.amazon.com/bedrock/) is a fully managed, serverless\n",
      "service, that provides users access to FMs from several third-party providers\n",
      "and from Amazon through a single API. After you select a FM to use you can\n",
      "privately customize it and connect your propriety data sources and knowledge\n",
      "bases.\n",
      "\n",
      "### Bedrock Supported Models\n",
      "\n",
      "Model Family | Provider | Type | Functionalities | Max Request  \n",
      "---|---|---|---|---  \n",
      "[Claude 3 Opus](https://aws.amazon.com/bedrock/claude/) | Anthropic | LLM | Advanced analysis of charts and graphs, brainstorming and hypothesis generation, coding, financials and market trends, forecasting, task automation, and research review. | 200k tokens  \n",
      "[Claude Sonnet (3, 3.5)](https://aws.amazon.com/bedrock/claude/) | Anthropic | LLM | RAG/search and retrieval. Code generation, forecasting, text from images, quality control, product recommendations, and targeted marketing. | 200k tokens  \n",
      "[Claude 3 Haiku](https://aws.amazon.com/bedrock/claude/) | Anthropic | LLM | Quick and accurate support in live interactions, content moderation, inventory management, knowledge extraction, optimize logistics, and translations. | 200k tokens  \n",
      "[Claude (2.0, and 2.1)](https://aws.amazon.com/bedrock/claude/) | Anthropic | LLM | Editing, outlining searching, Q&A, summarizing, and writing text. Automated workflows, coding, complex reasoning, content generation, comparison and analysis of documents, dialogue, detailed instruction, and trend forecasting. | 100-200k tokens  \n",
      "[Claude Instant](https://aws.amazon.com/bedrock/claude/) | Anthropic | LLM | Casual dialogue, comprehension, document comprehension, summarization, and text analysis. | 100k tokens  \n",
      "[Command (R, R+)](https://aws.amazon.com/bedrock/cohere-command-embed/) | Cohere | LLM | Chat, knowledge assistants, Q&A, RAG, text generation, and text summarization. | 128K  \n",
      "[Command](https://aws.amazon.com/bedrock/cohere-command/) | Cohere | LLM | Chat, Q&A, summarization, and text generation. | 4k tokens  \n",
      "[Command Light](https://aws.amazon.com/bedrock/cohere-command/) | Cohere | LLM | Chat, Q&A, summarization, and text generation. | 4k tokens  \n",
      "[Embed (English, Multilingual)](https://aws.amazon.com/bedrock/cohere-command/) | Cohere | Embeddings | Classification, clustering, RAG, and semantic search. | 1k dimensions  \n",
      "[Jamba-Instruct](https://aws.amazon.com/bedrock/ai21/) | AI21 Labs | LLM | Document summarization, instruction following, Q&A, and text generation. | 256k tokens  \n",
      "[Jurassic-2 Ultra](https://aws.amazon.com/bedrock/jurassic/) | AI21 Labs | LLM | Advanced information extraction, draft generation, intricate ideation, Q&A, and summarization. | 8k tokens  \n",
      "[Jurassic-2 Mid](https://aws.amazon.com/bedrock/jurassic/) | AI21 Labs | LLM | Advanced information extraction, draft generation, ideation, Q&A, and summarization. | 8k tokens  \n",
      "[Llama 70B (2, 3)](https://aws.amazon.com/bedrock/llama/) | Meta | LLM | Code generation, dialogue systems, instruction following, language modeling, sentiment analysis and nuance reasoning, text classification and nuance, and text summarization and accuracy. | 4-8k tokens  \n",
      "[Llama 3 8B](https://aws.amazon.com/bedrock/llama/) | Meta | LLM | Language translation, sentiment analysis, text classification, and text summarization. | 8k tokens  \n",
      "[Llama 2 13b](https://aws.amazon.com/bedrock/llama/) | Meta | LLM | Sentiment analysis, text classification, and language translation. | 4k tokens  \n",
      "[Mistral Large](https://aws.amazon.com/bedrock/mistral/) | Mistral | LLM | Code generation, complex multilingual reasoning tasks, instruction following, math and coding tasks, text summarization, and translation. | 32k tokens  \n",
      "[Mistral Small](https://aws.amazon.com/bedrock/mistral/) | Mistral | LLM | Classification, customer support, or text generation. | 32k tokens  \n",
      "[Mistral 7B](https://aws.amazon.com/bedrock/mistral/) | Mistral | LLM | Code completion, question answering, structuration, and text summarization. | 32k tokens  \n",
      "[Mixtral 8x7B](https://aws.amazon.com/bedrock/mistral/) | Mistral | LLM | Code completion, question answering, structuration, and text summarization. | 32k tokens  \n",
      "[Stable Diffusion XL 1.0](https://aws.amazon.com/bedrock/stable-diffusion/) | Stability AI | Image Generation | Generate, edit, or update images from text. | 77 tokens  \n",
      "[Titan Text Premier](https://aws.amazon.com/bedrock/titan/) | Amazon | LLM | Agents, brainstorming, chain of thought, chat, code generation, data formatting, extraction, open-ended text generation, paraphrasing, Q&A, RAG, rewriting, summarization, table creation. | 32k tokens  \n",
      "[Titan Text Express](https://aws.amazon.com/bedrock/titan/) | Amazon | LLM | Chain of thought, chat, code generation, data formatting, extraction, paraphrasing, Q&A, rewrite, retrieval augmented generation, summarization, table creation, and text generation. | 8k tokens  \n",
      "[Titan Text Lite](https://aws.amazon.com/bedrock/titan/) | Amazon | LLM | Chain of thought, chat, code generation, data formatting, extraction, paraphrasing, Q&A, rewrite, summarization, table creation, and text generation. | 4k tokens  \n",
      "[Titan Image Generator](https://aws.amazon.com/bedrock/titan/) | Amazon | Image Generation | Text to image generation, editing, and variations. | 77 tokens  \n",
      "[Titan Multimodal Embeddings](https://aws.amazon.com/bedrock/titan/) | Amazon | Embeddings | Personalization, recommendations, and searching by image and/or text. | 8k tokens  \n",
      "[Titan Text Embeddings (1, 2)](https://aws.amazon.com/bedrock/titan/) | Amazon | Embeddings | Clustering, semantic similarity, and text retrieval. | 8k tokens  \n",
      "  \n",
      "Table of supported Bedrock models (updated 6/28/24). There may be several\n",
      "versions in each model family, check the models page for specifics and the\n",
      "most up-to-date information.\n",
      "\n",
      "## Azure OpenAI\n",
      "\n",
      "[Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-\n",
      "services/openai/overview) is a partnership between Azure and OpenAI that\n",
      "enables Azure users to use OpenAI via an API, Python SDK, or their web-based\n",
      "interface while authenticated with their Azure cloud credentials. Azure OpenAI\n",
      "distinguishes itself from OpenAI by offering co-developed APIs, enhanced\n",
      "security, and private networking. Throughout this article, the term “OpenAI”\n",
      "refers exclusively to Azure OpenAI for the sake of brevity.\n",
      "\n",
      "### OpenAI Supported Models\n",
      "\n",
      "Model Family | Type | Functionalities | Max Request  \n",
      "---|---|---|---  \n",
      "[Embeddings (Ada, Text Embedding 3 (Small, Large))](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#embeddings-models) | Embeddings | Anomaly detection, classification tasks, clustering, recommendations, and searching. | 2k-8k tokens  \n",
      "[DALL-E (2 and 3)](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#dall-e) | Image Generation | Generate, edit, or update images from text. | 1k-4k characters  \n",
      "[GPT Base (Babbage-002 and Davinci-002)](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#fine-tuning-models) | Base | Generate and understand text or code. Not trained with instruction following. | 16k tokens  \n",
      "[GPT-3.5 Turbo](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-35) | LLM | Advanced complex reasoning and chat, code understanding and generation, and traditional completions tasks. | 4k and 16k tokens  \n",
      "[GPT-4](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-4) | LLM | Advanced complex reasoning and chat, advanced problem solving, code understanding and generation, and traditional completions tasks. | 8k and 32k tokens  \n",
      "[GPT-4 Turbo](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-4o-and-gpt-4-turbo) | LLM | GPT-4 capabilities, instruction following, and parallel function calling. | 128k tokens  \n",
      "[GPT-4 Turbo with Vision](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#deploying-gpt-4-turbo-with-vision-ga) | LLM | GPT-4 Turbo capabilities, image analysis, and Q&A. | 128k tokens  \n",
      "[GPT-4o](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-4o-and-gpt-4-turbo) | LLM | GPT-4 Turbo with Vision capabilities. Faster responses. | 128k tokens  \n",
      "[Whisper](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#whisper-models-preview) | Transcription | Convert audio to text. | 25 MB audio size  \n",
      "  \n",
      "Table of supported Azure OpenAI models (updated 6/28/24). Check the models\n",
      "page for specifics and the most up-to-date information.\n",
      "\n",
      "## Amazon Bedrock vs Azure OpenAI Functionality\n",
      "\n",
      "OpenAI certainly has a lot of name recognition. Due to this people have a\n",
      "conception that it is leaps and bounds ahead of other Generative AI services.\n",
      "However, as Randall Hunt, VP of Cloud Strategy and Innovation at Caylent,\n",
      "relayed on in Yan Cui’s [Real-World Serverless\n",
      "podcast](https://www.youtube.com/watch?v=jxL5Zk12JUE), “There wasn’t anything\n",
      "crazy great about the way OpenAI did anything, it just happened to be one of\n",
      "the first times we could see the power of these LLMs through an interface.”\n",
      "Still, GPT-4o is generally recognized [as the leader in terms of pure\n",
      "quality](https://arxiv.org/pdf/2307.13383.pdf).\n",
      "\n",
      "### Service Comparison\n",
      "\n",
      "Let’s compare some functionality on the service and model level to see how\n",
      "they fare. As this is the cloud, when comparing Bedrock to OpenAI we need to\n",
      "consider things like supported regions and security.\n",
      "\n",
      "  * **Documentation/Community:** Documentation and community support are challenging to quantify precisely, but based on anecdotal assessments, it’s fair to say that the documentation for both services is satisfactory at best. There is a lot of missing information and instructions that are all over the place. This is likely because both services and the models within the services are so new and constantly changing.\n",
      "  * **No-Code Playgrounds:** While both services are accessible via APIs and SDK as well, a no-code playground can be a helpful interface to utilize some of the models.\n",
      "\n",
      "![Amazon Bedrock Playground](https://assets.vantage.sh/blog/bedrock-vs-\n",
      "openai/bedrock-playground.png)\n",
      "\n",
      "[Amazon Bedrock Playground](https://aws.amazon.com/blogs/aws/amazon-bedrock-\n",
      "is-now-generally-available-build-and-scale-generative-ai-applications-with-\n",
      "foundation-models/)\n",
      "\n",
      "![Azure AI Studio Playground](https://assets.vantage.sh/blog/bedrock-vs-\n",
      "openai/azure-playground.png)\n",
      "\n",
      "[Azure AI Studio Playground](https://learn.microsoft.com/en-us/azure/ai-\n",
      "services/openai/chatgpt-quickstart)\n",
      "\n",
      "  * **Provisioned Throughput:** Bedrock offers a Provisioned Throughput payment plan for some model types that is advantageous for large workloads.\n",
      "\n",
      "### Model Comparison\n",
      "\n",
      "(Updated 1/9/24)\n",
      "\n",
      "Similarly, there are several factors to take into consideration when comparing\n",
      "models within their respective categories, such as max tokens, supported\n",
      "languages, and training data date. Later, we’ll go in-depth on pricing and\n",
      "performance.\n",
      "\n",
      "  * **Max Tokens:** Max tokens range for model categories and types. \n",
      "    * Embeddings Models: Bedrock and OpenAI both have models with 8k tokens.\n",
      "    * Image Generation Models: Characters to tokens [vary](https://platform.openai.com/tokenizer), however, 1000 characters correspond to roughly 250 tokens. So DALL-E has more tokens than Stable Diffusion and Titan Image Generator.\n",
      "    * LLMs: Both Bedrock and OpenAI provide models with 4k and 8k token options. OpenAI extends the range with 16k and 32k tokens. Bedrock and OpenAI offer models with 100k and 128k tokens, respectively, (corresponding to an impressive [300 pages](https://help.openai.com/en/articles/8555510-gpt-4-turbo)). However, the Claude model takes the trophy with an impressive maximum capacity of 200k tokens. That corresponds to [500 pages of information](https://www.anthropic.com/index/claude-2-1-prompting).\n",
      "  * **Supported Regions:** [Bedrock is available](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html#bedrock-regions) in Asia Pacific (Singapore), Asia Pacific (Tokyo), AWS GovCloud (US-West), Europe (Frankfurt), US East (N. Virginia), and US West (Oregon). OpenAI regions vary per model as follows:\n",
      "\n",
      "Region | Supported Models  \n",
      "---|---  \n",
      "Australia East | Ada, GPT-3.5 Turbo, GPT-4, GPT-4 With Vision  \n",
      "Canada East | Ada, GPT-3.5 Turbo, GPT-4  \n",
      "East US | Ada, DALL-E, GPT-3.5 Turbo, GPT-4  \n",
      "East US 2 | Ada, GPT-3.5 Turbo, GPT-4  \n",
      "France Central | Ada, GPT-3.5 Turbo, GPT-4  \n",
      "Japan East | Ada, GPT-3.5 Turbo, GPT-4  \n",
      "North Central US | Ada, Base Models, GPT-3.5 Turbo, Whisper  \n",
      "Norway East | Ada, GPT-4  \n",
      "South Central US | Ada, GPT-3.5 Turbo, GPT-4  \n",
      "South India | GPT-3.5 Turbo, GPT-4  \n",
      "Sweden Central | Ada, Base Models, DALL-E, GPT-3.5 Turbo, GPT-4.5  \n",
      "Switzerland North | Ada, GPT-3.5 Turbo, GPT-4  \n",
      "UK South | Ada, GPT-3.5 Turbo, GPT-4  \n",
      "West Europe | Ada, GPT-3.5 Turbo, Whisper  \n",
      "West US | Ada, GPT-3.5 Turbo, GPT-4  \n",
      "  \n",
      "Azure OpenAI supported models by region (as of 1/9/24). Note that model\n",
      "availability may vary based on the region and service, and there may be\n",
      "certain limitations in some regions.\n",
      "\n",
      "  * **Supported Languages:** Bedrock language capacity is model-specific. Command, Embed - English, Llama 2, Stable Diffusion, Titan Image Generator, Titan Multimodal Embeddings, and Titan Text Lite support only English. Jurassic supports 7 languages, Claude supports [12+](https://support.anthropic.com/en/articles/7996845-what-are-some-things-i-can-use-claude-for), Titan Embeddings supports 25+, while Embed - Multilingual and Titan Text Express support over 100. See the model pages for the specific languages. OpenAI has less information available on which languages are supported, however, [this response](https://help.openai.com/en/articles/6742369-how-do-i-use-the-openai-api-in-different-languages) claims it is available for use in a variety of languages.\n",
      "  * **Training Data Date:** OpenAIs models Ada, GPT-3.5 Turbo, GPT-4, and the Base Models are trained until Sept 2021. GPT-4 Turbo is trained until Apr 2023. Bedrock’s training data date was a little harder to find, we had to go to the retrospective provider’s sites to find that the only publicly available dates are [Claude’s (December 2022)](https://support.anthropic.com/en/articles/8114494-how-up-to-date-is-claude-s-training-data) and [Jurassic’s (current up to mid-2022).](https://www.ai21.com/blog/introducing-j2)\n",
      "\n",
      "## Bedrock Pricing\n",
      "\n",
      "[Charges for Bedrock](https://aws.amazon.com/bedrock/pricing/) are applied for\n",
      "model inference and customization. There are two plans available for model\n",
      "inference, On-Demand and Provisioned Throughput. Model customization and\n",
      "Provisioned Throughput are not available for all models. Price varies per\n",
      "region.\n",
      "\n",
      "### On-Demand\n",
      "\n",
      "The non-committal, pay-by-usage option. Charges vary depending on the model\n",
      "type. Text generation models incur charges per input token processed and\n",
      "output token generated. Embeddings models charge per input token processed.\n",
      "Image generation models charge per image generated.\n",
      "\n",
      "Model | Price per 1000 Input Tokens | Price per 1000 Output Tokens  \n",
      "---|---|---  \n",
      "Claude 3.5 Sonnet | $0.003 | $0.015  \n",
      "Claude 3 Opus | $0.015 | $0.075  \n",
      "Claude 3 Sonnet | $0.003 | $0.015  \n",
      "Claude 3 Haiku | $0.00025 | $0.00125  \n",
      "Claude 2.1 | $0.008 | $0.024  \n",
      "Claude 2.0 | $0.008 | $0.024  \n",
      "Claude Instant | $0.0008 | $0.0024  \n",
      "Command R+ | $0.0030 | $0.0150  \n",
      "Command R | $0.0005 | $0.0015  \n",
      "Command | $0.0015 | $0.002  \n",
      "Command Light | $0.0003 | $0.0006  \n",
      "Embed – English | $0.0001 | N/A  \n",
      "Embed – Multilingual | $0.0001 | N/A  \n",
      "Jamba-Instruct | $0.0005 | $0.0007  \n",
      "Jurassic-2 Ultra | $0.0188 | $0.0188  \n",
      "Jurassic-2 Mid | $0.0125 | $0.0125  \n",
      "Llama 3 70B | $0.00265 | $0.0035  \n",
      "Llama 3 8B | $0.0003 | $0.0006  \n",
      "Llama 2 70B | $0.00195 | $0.00256  \n",
      "Llama 2 Chat (13B) | $0.00075 | $0.001  \n",
      "Mistral 7B | $0.00015 | $0.0002  \n",
      "Mixtral 8x7B | $0.00045 | $0.0007  \n",
      "Mistral Small | $0.001 | $0.003  \n",
      "Mistral Large | $0.004 | $0.012  \n",
      "Titan Text Premier | $0.0005 | $0.0015  \n",
      "Titan Text Express | $0.0002 | $0.0006  \n",
      "Titan Text Lite | $0.00015 | $0.0002  \n",
      "Titan Multimodal Embeddings | $0.0008 ($0.00006 per input image) | N/A  \n",
      "Titan Text Embeddings | $0.0001 | N/A  \n",
      "Titan Text Embeddings V2 | $0.00002 | N/A  \n",
      "  \n",
      "Bedrock On Demand LLMs pricing table. (Updated 6/28/24).\n",
      "\n",
      "Model | Image resolution | Cost Per Image - Standard Quality (<51 steps) | Cost Per Image - Premium Quality (>51 steps)  \n",
      "---|---|---|---  \n",
      "SDXL 0.8 (Stable Diffusion) | 512X512 or smaller | $0.018 | $0.036  \n",
      "| Larger than 512X512 | $0.036 | $0.072  \n",
      "SDXL 1.0 (Stable Diffusion) | 1024X1024 or smaller | $0.04 | $0.08  \n",
      "Titan Image Generator (Standard) | 512X512 | $0.008 | $0.01  \n",
      "| 1024X1024 | $0.01 | $0.012  \n",
      "Titan Image Generator (Custom Models) | 512X512 | $0.018 | $0.02  \n",
      "| 1024X1024 | $0.02 | $0.022  \n",
      "  \n",
      "Bedrock On Demand Image Generation Models pricing table. (Updated 6/28/24).\n",
      "\n",
      "### Provisioned Throughput\n",
      "\n",
      "You have the option to buy model units (specific throughput measured by the\n",
      "maximum number of input/output tokens processed per minute) for a specific\n",
      "model (including custom models). Pricing is charged hourly and you can choose\n",
      "a one-month or six-month term. This pricing model is best suited for “large\n",
      "consistent inference workloads that need guaranteed throughput.”\n",
      "\n",
      "Model | Price per Hour per Model Unit With No Commitment (Max One Custom Model Unit Inference) | Price per Hour per Model Unit With a One Month Commitment (Includes Inference) | Price per Hour per Model Unit With a Six Month Commitment (Includes Inference)  \n",
      "---|---|---|---  \n",
      "Claude 2.0/2.1 | $70.00 | $63.00 | $35.00  \n",
      "Claude Instant | $44.00 | $39.60 | $22.00  \n",
      "Command | $49.50 | $39.60 | $23.77  \n",
      "Command Light | $8.56 | $6.85 | $4.11  \n",
      "Embed - English | $7.12 | $6.76 | $6.41  \n",
      "Embed - Multilingual | $7.12 | $6.76 | $6.41  \n",
      "Llama 2 Pre-Trained and Chat (13B) | N/A | $21.18 | $13.08  \n",
      "Llama 2 Pre-Trained (70B) | N/A | $21.18 | $13.08  \n",
      "SDXL1.0 (Stable Diffusion) | N/A |  $49.86 | $46.18  \n",
      "Titan Embeddings | N/A | $6.40 | $5.10  \n",
      "Titan Image Generator (Standard) | N/A | $16.20 | $13.00  \n",
      "Titan Image Generator (Custom Models) | $23.40 | $21.00 | $16.85  \n",
      "Titan Multimodal Embeddings | $9.38 | $8.45 | $6.75  \n",
      "Titan Text Lite | $7.10 | $6.40 | $5.10  \n",
      "Titan Text Express | $20.50 | $18.40 | $14.80  \n",
      "  \n",
      "Bedrock Provisioned Throughput Models pricing table. (Updated 6/28/24).\n",
      "\n",
      "### Model Customization\n",
      "\n",
      "You’re charged for model customization based on the number of processed tokens\n",
      "and model storage. Keep in mind, inference on more than one model unit is only\n",
      "available for Provisioned Throughput.\n",
      "\n",
      "Model | Price to Train 1000 Tokens or Price per Image Seen | Price for Storage per Custom Model per Month | Price to Infer from a Custom Model for One Model Unit per Hour with No Commitment  \n",
      "---|---|---|---  \n",
      "Command | $0.004 | $1.95 | $49.50  \n",
      "Command Light | $0.001 | $1.95 | $8.56  \n",
      "Llama 2 Pre-trained (13B) | $0.00149 | $1.95 | $23.50  \n",
      "Llama 2 Pre-trained (70B) | $0.00799 | $1.95 | $23.50  \n",
      "Titan Image Generator | $0.005 | $1.95 | $23.40  \n",
      "Titan Multimodal Embeddings | $0.0002 | $1.95 | $9.38  \n",
      "Titan Text Lite | $0.0004 | $1.95 | $7.10  \n",
      "Titan Text Express | $0.008 | $1.95 | $20.50  \n",
      "  \n",
      "Bedrock Model Customization pricing table. (Updated 6/28/24).\n",
      "\n",
      "## Azure OpenAI Pricing\n",
      "\n",
      "[Charges for OpenAI](https://azure.microsoft.com/en-\n",
      "us/pricing/details/cognitive-services/openai-service/) are fairly simple. It\n",
      "is a pay-as-you-go, with no commitment. There are additional customization\n",
      "charges. Price varies per region.\n",
      "\n",
      "### Pay-As-You-Go\n",
      "\n",
      "Charges vary for different model types and if applicable, context. Text\n",
      "generation models charge per prompt tokens and completion tokens. Embeddings\n",
      "models and base models charge per usage tokens. Image generation models charge\n",
      "per 100 images generated.\n",
      "\n",
      "Model | Context | Price per 1000 Input Tokens | Price per 1000 Output Tokens  \n",
      "---|---|---|---  \n",
      "Ada | N/A | $0.0001 | N/A  \n",
      "Text Embedding 3 Large | N/A | $0.00013 | N/A  \n",
      "Text Embedding 3 Small | N/A | $0.00002 | N/A  \n",
      "Babbage-002 (GPT Base) | N/A | $0.0004 | $0.0004  \n",
      "Davinci-002 (GPT Base) | N/A | $0.002 | $0.002  \n",
      "GPT-3.5 Turbo Instruct | 4k | $0.0015 | $0.002  \n",
      "GPT-3.5 Turbo | 16k | $0.0005 | $0.0015  \n",
      "GPT-4 | 8k | $0.03 | $0.06  \n",
      "GPT-4 | 32k | $0.06 | $0.12  \n",
      "GPT-4 Turbo | 128k | $0.01 | $0.03  \n",
      "GPT-4 Turbo With Vision | 128k | $0.01 | $0.03  \n",
      "GPT-4o | 128k | $0.005 | $0.015  \n",
      "  \n",
      "OpenAI Pay-As-You-Go LLMs pricing table. (Updated 6/28/24).\n",
      "\n",
      "Models | Quality | Resolution | Price (per 100 Images)  \n",
      "---|---|---|---  \n",
      "Dall-E-3 | Standard | 1024X1024 | $4  \n",
      "| Standard | 1024X1792, 1792X1024 | $8  \n",
      "Dall-E-3 | HD | 1024X1024 | $8  \n",
      "| HD | 1024X1792, 1792X1024 | $12  \n",
      "Dall-E-2 | Standard | 1024X1024 | $2  \n",
      "  \n",
      "OpenAI Pay-As-You-Go Image Generation Model (Dall-E) pricing table. (Updated\n",
      "6/28/24).\n",
      "\n",
      "Models | Price  \n",
      "---|---  \n",
      "Whisper | $0.36 per Hour  \n",
      "TTS (Text to Speech) | $15 per 1M Characters  \n",
      "TTS HD | $30 per 1M Characters  \n",
      "  \n",
      "OpenAI Pay-As-You-Go Speech Generation Model (Whisper) pricing table. (Updated\n",
      "6/28/24).\n",
      "\n",
      "### Model Customization\n",
      "\n",
      "Model customization charges are based on training time and hosting time with\n",
      "slightly different pricing per region.\n",
      "\n",
      "Model | Training per 1000 tokens | Price for Hosting per Hour  \n",
      "---|---|---  \n",
      "Babbage-002 (GPT Base) | $0.0004 | $1.70  \n",
      "Davinci-002 (GPT Base) | $0.006 | $3  \n",
      "GPT-3.5 Turbo (4k) | $0.008 | $3  \n",
      "GPT-3.5 Turbo (16k) | $0.008 | $3  \n",
      "  \n",
      "OpenAI Model Customization pricing table. (Updated 6/28/24).\n",
      "\n",
      "## Pricing Comparison: Bedrock vs OpenAI\n",
      "\n",
      "(Updated 1/9/24)\n",
      "\n",
      "On a model-by-model comparison, Bedrock is most often cheaper than OpenAI.\n",
      "However, cost does not tell the full story and the scenarios below are based\n",
      "purely on an analysis of pricing.\n",
      "\n",
      "### Standard Context Window: Command, Llama 2, Titan Text vs GPT-3.5 Turbo 4k\n",
      "\n",
      "For a lower capacity model where we want to perform tasks such as chat,\n",
      "summarization on an article-length passage, Q&A, etc, we can consider one of\n",
      "the models with a 4k token max. There is one model from OpenAI that fits the\n",
      "criteria—GPT-3.5 Turbo 4k, and multiple of Bedrock—Command, Llama 2, Titan\n",
      "Text Lite, and Titan Text Express. The pricing for Command and GPT-3.5 Turbo\n",
      "4k are the same, at $0.0015 per 1000 input tokens and $0.002 per 1000 output\n",
      "tokens.\n",
      "\n",
      "Titan Text Lite, which can perform many of the same capabilities is much\n",
      "cheaper at $0.00015 per 1000 input tokens and$0.0002 per 1000 output tokens.\n",
      "Llama 2 is $0.00075 per 1000 input tokens and $0.001 per 1000 output tokens.\n",
      "Another option is Titan Text Express, the difference between the Lite version\n",
      "is that it has retrieval augmented generation ability and a maximum of 8k\n",
      "tokens. The price is $0.0008 per 1000 input tokens and $0.0016 per 1000 output\n",
      "tokens, cheaper than GPT-3.5 Turbo 4k.\n",
      "\n",
      "#### Chatbot Scenario: Titan Text Express vs GPT-3.5 Turbo 4k\n",
      "\n",
      "Consider a scenario where you want to develop a simple customer service\n",
      "chatbot. The chatbot will need to be able to handle customer inquiries,\n",
      "provide assistance, and answer questions on a range of topics related to your\n",
      "products and services. The model will need to handle short sentences as well\n",
      "as more detailed discussions.\n",
      "\n",
      "A standard question could be about 15 tokens and the answer could be 85. If\n",
      "your chatbot is answering 250,000 similar tokened questions a month the\n",
      "estimated price would be:\n",
      "\n",
      "15 tokens X 250,000 questions = 3,750,000 input tokens\n",
      "\n",
      "85 tokens X 250,000 answers = 21,250,000 output tokens\n",
      "\n",
      "**Titan Text Express** : 3,750,000 input tokens / 1000 X $0.0008 + 21,250,000\n",
      "output tokens / 1000 X $0.0016 = $37\n",
      "\n",
      "**GPT-3.5 Turbo 4k** : 3,750,000 input tokens / 1000 X $0.0015 + 21,250,000\n",
      "output tokens / 1000 X $0.002 = $48\n",
      "\n",
      "GPT-3.5 Turbo 4k is 30% more than Titan Text Express, making Bedrock the\n",
      "cheaper option for a lower-capacity model.\n",
      "\n",
      "### Long Context Window: Long Context Window: Jurassic-2 vs GPT-4 8k\n",
      "\n",
      "For more advanced tasks such as advanced information extraction, draft\n",
      "generation, and summarization on larger passages, let’s compare some of the\n",
      "models with an 8k token max. Between the Jurassic-2 models the Ultra model\n",
      "stands out because of intricate ideation, and therefore compares well with\n",
      "GPT-4 8k. Jurassic-2 Ultra is much cheaper at $0.0188 per 1000 input tokens\n",
      "and $0.0188 per 1000 output tokens when compared to GPT-4 8k’s $0.03 per 1000\n",
      "input tokens and $0.06 per 1000 output tokens.\n",
      "\n",
      "### Long Context Window: Claude Instant vs GPT-3.5 Turbo 16k\n",
      "\n",
      "For even larger tasks consider Claude Instant (100k token max) and GPT-3.5\n",
      "Turbo 16k (16k token max). The capabilities and pricing are relatively\n",
      "similar. However, the choice is much more case-dependent since with Claude\n",
      "Instant you are charged $0.0008 per 1000 input tokens and $0.0024 per 1000\n",
      "output tokens, compared to $0.003 per 1000 input tokens and $0.004 per 1000\n",
      "output tokens for GPT-3.5 Turbo 16k. So, because of the lower pricing, Claude\n",
      "Instant would be a great choice with a higher token input amount and lower\n",
      "output token amount.\n",
      "\n",
      "### Extra Long Context Window: Claude 2.1 vs GPT-4 32k\n",
      "\n",
      "For high-capacity models with very advanced tasks, such as content generation\n",
      "and complex reasoning, consider Claude 2.1 vs GPT-4 32k. Claude 2.1 has an\n",
      "impressive maximum of 100k tokens, while GPT-4 32k provides 32k tokens. Claude\n",
      "2.1 is a great choice since it is way cheaper at $0.008 per 1000 input tokens\n",
      "and $0.024 per 1000 output tokens. GPT-4 32k is $0.06 per 1000 input tokens\n",
      "and $0.12 per 1000 output tokens.\n",
      "\n",
      "#### Text Summarization Scenario: Claude 2.1 vs GPT-4 32k\n",
      "\n",
      "You work at a content creation agency and need to summarize lengthy articles\n",
      "and reports for clients. You want to process articles at around 25,000 tokens\n",
      "and summarize them to about 5,000 tokens. If you process 300 articles a month\n",
      "consider the estimated prices:\n",
      "\n",
      "25,000 tokens X 300 articles = 7,500,000 input tokens\n",
      "\n",
      "5,000 tokens X 300 responses = 1,500,000 output tokens\n",
      "\n",
      "**Claude 2.1** : 7,500,000 input tokens / 1000 X $0.008 + 1,500,000 output\n",
      "tokens / 1000 X $0.024 = $96\n",
      "\n",
      "**GPT-4 32k** : 7,500,000 input tokens / 1000 X $0.06 + 1,500,000 output\n",
      "tokens / 1000 X $0.12 = $630\n",
      "\n",
      "Claude 2.1 is remarkably cost-effective, positioning Bedrock as the more\n",
      "economical choice, with 556% cost savings in this situation.\n",
      "\n",
      "In terms of functionality, [this article](https://medium.com/version-1/aws-\n",
      "bedrocks-claude-2-100k-vs-azure-openai-s-gpt-4-32k-a-comparative-\n",
      "analysis-96e3eb9fd05a) does a great in-depth comparison of the the\n",
      "functionality of the two models and concludes that GPT-4 32k performs slightly\n",
      "better. Some takeaways are a similar performance for code generation and\n",
      "conversion, a better performance for GPT-4 32k’s dataset analysis and math\n",
      "skills, and Claude’s 2.1 distinct ability to summarize text over 32k tokens.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "There are a number of dimensions to consider when comparing Bedrock and\n",
      "OpenAI, such as region availability, tokens, model quality, and price. Based\n",
      "on the variety of models, lower price, and large token max from the Claude\n",
      "Model, we think Bedrock is increasingly competitive for applications where the\n",
      "absolute best performance is not required.\n",
      "\n",
      "![](/images/vantage-logomark.svg)\n",
      "\n",
      "Lower your AWS costs.\n",
      "\n",
      "Get Started\n",
      "\n",
      "Create and monitor cost efficient AI with\n",
      "[Vantage](https://console.vantage.sh/signup).\n",
      "\n",
      "![](/images/icons/slack-icon.svg)\n",
      "\n",
      "**Join our Slack Group**\n",
      "\n",
      "Interested in this type of content? Join our growing  \n",
      "Slack community of over 1,000 cloud professionals\n",
      "\n",
      "[Join Slack Group](https://vantage.sh/slack)\n",
      "\n",
      "###### More from the blog\n",
      "\n",
      "[ ![](https://assets.vantage.sh/blog/aws-lambda-avoid-infinite-loops/aws-\n",
      "lambda-avoid-infinite-loops.jpg) ](/blog/aws-lambda-avoid-infinite-loops)\n",
      "\n",
      "Content •  Oct 25, 2024\n",
      "\n",
      "[Recursive AWS Lambda Horror Stories and How to Avoid Them](/blog/aws-lambda-\n",
      "avoid-infinite-loops)\n",
      "\n",
      "Infinite recursion in Lambda can cause costs to spiral out of control.\n",
      "Following best practices can help you avoid getting caught up in your own\n",
      "Lambda horror story.\n",
      "\n",
      "[ ![](https://assets.vantage.sh/blog/gcs-vs-r2-cost/gcs-vs-r2-cost.jpg)\n",
      "](/blog/gcs-vs-r2-cost)\n",
      "\n",
      "Content •  Oct 21, 2024\n",
      "\n",
      "[Google Cloud Storage vs Cloudflare R2: Pricing Comparison](/blog/gcs-\n",
      "vs-r2-cost)\n",
      "\n",
      "For frequently accessed data, R2 is always less expensive than GCS, while GCS\n",
      "is much more cost-effective for archival use cases.\n",
      "\n",
      "[ ![](https://assets.vantage.sh/blog/aws-graviton-vs-intel/aws-graviton-vs-\n",
      "intel.jpg) ](/blog/aws-graviton-vs-intel)\n",
      "\n",
      "Content •  Oct 11, 2024\n",
      "\n",
      "[The Rise of Graviton at AWS and How You Can Save by Switching](/blog/aws-\n",
      "graviton-vs-intel)\n",
      "\n",
      "While Graviton processors are gaining significant adoption, some companies are\n",
      "still hesitant to make the switch. Switching to Graviton is a significant way\n",
      "to lower compute costs.\n",
      "\n",
      "![](/images/logo.svg)\n",
      "\n",
      "Tools for developers to analyze,  \n",
      "report on and reduce cloud costs.\n",
      "\n",
      "© VNTG Inc.\n",
      "\n",
      "  * [ ![](/images/icons/icon-twitter.png) ](https://twitter.com/joinvantage)\n",
      "  * [ ![](/images/icons/icon-linkedin.png) ](https://www.linkedin.com/company/vantage-sh/)\n",
      "  * [ ![](/images/icons/icon-github.png) ](https://github.com/vantage-sh/)\n",
      "  * [ ![](/images/icons/icon-youtube.png) ](https://www.youtube.com/@Vantagesh)\n",
      "\n",
      "###### Features\n",
      "\n",
      "  * [Integrations](/integrations)\n",
      "  * [Partners](/partners)\n",
      "  * [Cost Reports](/features/cost-reports)\n",
      "  * [Resource Reports](/features/resource-inventory)\n",
      "  * [Network Flow Reports](/features/network-flow-reports)\n",
      "  * [Usage-Based Reporting](/features/usage-based-reporting)\n",
      "  * [Segments](/features/segments)\n",
      "  * [Cost Anomaly Alerts](/features/cost-anomaly-alerts)\n",
      "  * [Financial Planning](/features/financial-planning)\n",
      "  * [Autopilot for AWS Savings Plans](/features/autopilot)\n",
      "  * [Budgets](/features/budgets)\n",
      "  * [Per Unit Costs](/features/unit-costs)\n",
      "  * [Issues](/features/issues)\n",
      "  * [Per-Resource Costs](/features/per-resource-costs)\n",
      "  * [Financial Commitment Reports](/features/financial-commitment-reports)\n",
      "  * [Cost Recommendations](/features/cost-recommendations)\n",
      "  * [Kubernetes Cost Visibility](/integrations/kubernetes)\n",
      "  * [Team Access and Management](/features/team-management)\n",
      "  * [Virtual Tagging](/features/virtual-tagging)\n",
      "  * [Terraform](/features/terraform)\n",
      "\n",
      "###### Documentation\n",
      "\n",
      "  * [Product Documentation](https://docs.vantage.sh)\n",
      "  * [Product Changelog](https://docs.vantage.sh/changelog)\n",
      "  * [API Documentation](https://vantage.readme.io/)\n",
      "  * [Vantage University](https://docs.vantage.sh/vantage_university)\n",
      "  * [Privacy Policy](/privacy-policy)\n",
      "  * [Website Terms of Use](/website-terms-of-use)\n",
      "  * [VNTG Service Agreement](/service-agreement)\n",
      "  * [Authorized Sub-Processors](/subprocessors)\n",
      "\n",
      "###### Community\n",
      "\n",
      "  * [Events](https://vantage.sh/events)\n",
      "  * [Slack Community](https://vantage.sh/slack)\n",
      "  * [EC2Instances.info](https://instances.vantage.sh/)\n",
      "  * [Cloud Cost Reports](/cloud-cost-report)\n",
      "  * [Cloud Cost Handbook](https://handbook.vantage.sh/)\n",
      "  * [Cloud Cost Leaderboard](https://leaderboard.vantage.sh/)\n",
      "\n",
      "###### Company\n",
      "\n",
      "  * [About](/about)\n",
      "  * [Blog](/blog)\n",
      "  * [Customers](/customers)\n",
      "  * [Pricing](/pricing)\n",
      "  * [Careers](/careers)\n",
      "  * [Podcasts and Talks](/podcasts)\n",
      "  * [Newsroom](/newsroom)\n",
      "  * [Branding](/branding)\n",
      "  * [Security](/security)\n",
      "  * [Vantage Status](https://status.vantage.sh/)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "def scrape_and_convert_to_markdown(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted elements (optional)\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Convert HTML to Markdown\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.ignore_images = False\n",
    "        markdown_content = h.handle(str(soup))\n",
    "        \n",
    "        return markdown_content\n",
    "    else:\n",
    "        return f\"Failed to retrieve the webpage. Status code: {response.status_code}\"\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.vantage.sh/blog/azure-openai-vs-amazon-bedrock-cost\"\n",
    "\n",
    "markdown_result = scrape_and_convert_to_markdown(url)\n",
    "print(markdown_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the markdown content to a file\n",
    "with open(\"output.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(markdown_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a tool with the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraperMarkdownTool(BaseTool):\n",
    "    \n",
    "    name = \"Web Scraper to Markdown\"\n",
    "    description = \"Useful for scraping content from a website and converting it to markdown format. Input should be a valid URL.\"\n",
    "\n",
    "    def _run(self, url: str) -> str:\n",
    "        try:\n",
    "            # Send a GET request to the URL\n",
    "            response = requests.get(url)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Parse the HTML content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Remove unwanted elements (optional)\n",
    "                for script in soup([\"script\", \"style\"]):\n",
    "                    script.decompose()\n",
    "                \n",
    "                # Convert HTML to Markdown\n",
    "                h = html2text.HTML2Text()\n",
    "                h.ignore_links = False\n",
    "                h.ignore_images = False\n",
    "                markdown_content = h.handle(str(soup))\n",
    "                \n",
    "                return markdown_content\n",
    "            else:\n",
    "                return f\"Failed to retrieve the webpage. Status code: {response.status_code}\"\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "    def _arun(self, url: str) -> str:\n",
    "        # This tool doesn't support async operations\n",
    "        raise NotImplementedError(\"WebScraperMarkdownTool does not support async\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price for 1000 output tokens for Claude Sonnet is $0.015.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# Create an instance of our custom tool\n",
    "web_scraper_tool = WebScraperMarkdownTool()\n",
    "\n",
    "# Initialize the agent with our custom tool\n",
    "tools = [web_scraper_tool]\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False)\n",
    "\n",
    "# Use the agent\n",
    "result = agent.run(\"Get the Claude Sonnet price for 1000 output token, see https://www.vantage.sh/blog/azure-openai-vs-amazon-bedrock-cost\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quick_start",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
